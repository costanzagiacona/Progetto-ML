{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/costanzagiacona/Progetto-ML/blob/main/pneumonia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Image classification**\n",
        "Machine learning project for image classification using different types of models. I used 'MedMNIST-Pneumonia' data set for training.\n",
        "The aim of this project is to train and evaluate one or more classification models in order to establish if the lungs in the images have pneumonia or not."
      ],
      "metadata": {
        "id": "1snjGbwWBgd5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rPAMbFvF-6Nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e82c9961-9cfb-4e74-b448-4450b7bff16c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell that groups all the libraries used for this project"
      ],
      "metadata": {
        "id": "ndEmumftBYVE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Sx7pWfpH5oqL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Imports for data manipulation and model preparation\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Imports for visualization and presenting results\n",
        "from IPython.display import Markdown, display\n",
        "import seaborn as sns\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "# Imports for modeling with Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras_tuner\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D\n",
        "from keras import regularizers\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "fesokz2cFqR3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I loaded the dataset. The images are stored in the file 'pneumonia_images' and the labels in 'pneumonia_labels'."
      ],
      "metadata": {
        "id": "ArSiFCY0q9uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading dataset MedMNIST\n",
        "try:\n",
        "    X = np.load(\"/content/data_set/pneumonia_images.npy\")\n",
        "    y = np.load(\"/content/data_set/pneumonia_labels.npy\")\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Make sure you have the correct path\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "01yqa9hOsIMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8fbfee5-fccc-431a-9ac7-67a6c0759155"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File not found. Make sure you have the correct path\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet counts the number of normal and pneumonia images, displays a sample of each class, and shows the class distribution."
      ],
      "metadata": {
        "id": "Mnc7hMuOvBE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the resolution of images\n",
        "print(f'Image shape: {X[0].shape}')\n",
        "\n",
        "# Count the number of images in each class\n",
        "normal_images_count = np.sum(y == 0)\n",
        "pneumonia_images_count = np.sum(y == 1)\n",
        "\n",
        "print(f'Total normal images: {normal_images_count}')\n",
        "print(f'Total pneumonia images: {pneumonia_images_count}')\n",
        "\n",
        "# Function to display some images\n",
        "def show_images(images, labels, label, title):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    indices = np.where(labels == label)[0][:6]  # Get the first 6 indices of the specified label\n",
        "    for i, idx in enumerate(indices):\n",
        "        plt.subplot(2, 3, i + 1)\n",
        "        plt.imshow(images[idx], cmap='gray')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Label: {label}')\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display some normal images\n",
        "show_images(X, y, 0, 'Normal')\n",
        "\n",
        "# Display some pneumonia images\n",
        "show_images(X, y, 1, 'Pneumonia')\n",
        "\n",
        "# Distribution of classes\n",
        "labels = ['Normal', 'Pneumonia']\n",
        "counts = [normal_images_count, pneumonia_images_count]\n",
        "data = pd.DataFrame({'Class': labels, 'Count': counts})\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x='Class', y='Count', data=data, palette='viridis', hue='Class', dodge=False, legend=False)\n",
        "plt.title('Class Distribution')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "rDrBZNmstfO-",
        "outputId": "fb9e6626-a8a1-4d48-e45c-b66b04396f2e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-72866fe79468>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check the resolution of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Image shape: {X[0].shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Count the number of images in each class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnormal_images_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I randomly split the dataset into training set, validation set and test set. The training set is used to train the different models, the validation set to calculate the errors on new predictions and the test set to verify again the performance of the best models on new data. The split I used is approximately: 70% training set, 15% validation set and 15% test set.\n",
        "\n",
        "I used the function train_test_split, offered by Scikit-learn for this task, to do a first split of the data and then I did a second split of the temporary set to create all the three sets."
      ],
      "metadata": {
        "id": "FMcoIYjfxTpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Division of dataset in training, validation e test set\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "MyuyK2ppsMcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flattening the labels in 1D array\n",
        "y_train = y_train.ravel()\n",
        "y_validation = y_validation.ravel()\n",
        "y_test = y_test.ravel()"
      ],
      "metadata": {
        "id": "9KoIuWuB0etk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I added a channel for the images, because the CNNs used require it."
      ],
      "metadata": {
        "id": "1rIaJQ7gitS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If the images are garyscale, add a channel\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_validation = np.expand_dims(X_validation, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)"
      ],
      "metadata": {
        "id": "zVgT-iexuJaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training set dimension:\", X_train.shape)\n",
        "print(\"Data type:\", X_train.dtype)\n",
        "num_samples, height, width, channels = X_train.shape\n",
        "print(\"Number of samples:\", num_samples)\n",
        "print(\"Image height:\", height)\n",
        "print(\"Image width:\", width)\n",
        "print(\"Image number of channels:\", channels)"
      ],
      "metadata": {
        "id": "Gt9kQIwGHef3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to apply some filters to the images. The blur can help identifying the features."
      ],
      "metadata": {
        "id": "i2k8X75vR47D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly select an image with label 0 (normal)\n",
        "index_label_0 = random.choice(np.where(y == 0)[0])\n",
        "image_label_0 = X[index_label_0]\n",
        "\n",
        "# Randomly select an image with label 1 (pneumonia)\n",
        "index_label_1 = random.choice(np.where(y == 1)[0])\n",
        "image_label_1 = X[index_label_1]\n",
        "\n",
        "# Apply Gaussian blur filter for noise reduction\n",
        "blurred_image_0 = cv2.GaussianBlur(image_label_0, (5, 5), 0)\n",
        "blurred_image_1 = cv2.GaussianBlur(image_label_1, (5, 5), 0)\n",
        "\n",
        "# Apply median blur filter for noise reduction\n",
        "median_filtered_image_0 = cv2.medianBlur(blurred_image_0, 5)\n",
        "median_filtered_image_1 = cv2.medianBlur(blurred_image_1, 5)\n",
        "\n",
        "# Create a 3x2 grid of subplots\n",
        "fig, axes = plt.subplots(3, 2, figsize=(8, 6))\n",
        "\n",
        "# First row: original images\n",
        "axes[0, 0].imshow(image_label_0, cmap='gray')\n",
        "axes[0, 0].set_title('Original Image label: 0')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "axes[0, 1].imshow(image_label_1, cmap='gray')\n",
        "axes[0, 1].set_title('Original Image label: 1')\n",
        "axes[0, 1].axis('off')\n",
        "\n",
        "# Second row: images with Gaussian filter\n",
        "axes[1, 0].imshow(blurred_image_0, cmap='gray')\n",
        "axes[1, 0].set_title('Gaussian Filter (label 0)')\n",
        "axes[1, 0].axis('off')\n",
        "\n",
        "axes[1, 1].imshow(blurred_image_1, cmap='gray')\n",
        "axes[1, 1].set_title('Gaussian Filter (label 1)')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "# Third row: images with median filter\n",
        "axes[2, 0].imshow(median_filtered_image_0, cmap='gray')\n",
        "axes[2, 0].set_title('Median Filter (label 0)')\n",
        "axes[2, 0].axis('off')\n",
        "\n",
        "axes[2, 1].imshow(median_filtered_image_1, cmap='gray')\n",
        "axes[2, 1].set_title('Median Filter (label 1)')\n",
        "axes[2, 1].axis('off')\n",
        "\n",
        "# Adjust the layout of the subplots to optimize spacing\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xfPf72RSzGSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest**"
      ],
      "metadata": {
        "id": "CSi8O9894LUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a Random Forest model using scikit-learn in Python. Use the technique of exhaustive parameter search via GridSearchCV to optimize the model's accuracy.\n",
        "After training, evaluate the model's performance using accuracy and calculate the RMSE (Root Mean Squared Error) as a measure of discrepancy between observed and predicted values. Finally, measure the processing time taken to train the model."
      ],
      "metadata": {
        "id": "WEqAIURz-15Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTVUxL6t6AQP"
      },
      "outputs": [],
      "source": [
        "# Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)  # 42 is the seed\n",
        "rf_param_grid = {'n_estimators': [100], 'max_depth': [None]} # 100 trees\n",
        "rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Training Random Forest\n",
        "start_time = time.time()\n",
        "rf_grid_search.fit(X_train.reshape(len(X_train), -1), y_train) # Random Forest requires flattened images\n",
        "end_time = time.time()\n",
        "\n",
        "# Considering which model is the best\n",
        "rf_model_best = rf_grid_search.best_estimator_\n",
        "rf_pred = rf_model_best.predict(X_validation.reshape(len(X_validation), -1))\n",
        "rf_accuracy = accuracy_score(y_validation, rf_pred)\n",
        "rf_conf_matrix = confusion_matrix(y_validation, rf_pred)\n",
        "rf_class_report = classification_report(y_validation, rf_pred)\n",
        "\n",
        "# Predictions for the test set\n",
        "rf_pred_test = rf_model_best.predict(X_test.reshape(len(X_test), -1))\n",
        "rf_accuracy_test = accuracy_score(y_test, rf_pred_test)\n",
        "rf_conf_matrix_test = confusion_matrix(y_test, rf_pred_test)\n",
        "rf_class_report_test = classification_report(y_test, rf_pred_test)\n",
        "\n",
        "# RMSE\n",
        "rf_rmse = mean_squared_error(y_validation, rf_pred, squared=False)\n",
        "\n",
        "# Processing time\n",
        "rf_processing_time = end_time - start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I considered some metrics to decide how this model worked.\n",
        "\n",
        "The evaluation have been made on the validation set and on the test set to guarantee the model is not overfitting the data.\n",
        "I obtained high scores for the validation set and similar scores for the test set, so I can says the model fits the data."
      ],
      "metadata": {
        "id": "QB9a8b7D_DYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Random Forest chosen:\")\n",
        "print(rf_model_best)\n",
        "print(\"\\nAccuracy Random Forest (validation):\", rf_accuracy)\n",
        "print(\"Classification Report Random Forest:\")\n",
        "print(rf_class_report)\n",
        "\n",
        "print(\"\\nAccuracy Random Forest (test):\", rf_accuracy_test)\n",
        "print(\"Classification Report Random Forest:\")\n",
        "print(rf_class_report_test)"
      ],
      "metadata": {
        "id": "1HznitK0JzR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a visual representation of the confusion matrix, which says that the most part of the images has been correctly classificated."
      ],
      "metadata": {
        "id": "lS-tYlADAE_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with a specified size\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Create the first subplot in a 1x2 grid\n",
        "plt.subplot(1, 2, 1)  # 1 row, 2 columns, subplot 1\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Random Forest (Validation)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "# Create the second subplot in the same 1x2 grid\n",
        "plt.subplot(1, 2, 2)  # 1 row, 2 columns, subplot 2\n",
        "sns.heatmap(conf_matrix_test, annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Random Forest (Test)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "# Adjust layout and show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sRUXGhXpvy8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convolutional Neural Network**"
      ],
      "metadata": {
        "id": "dDqnCtLn4rqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CNN model is defined using the Sequential API from Keras. It consists of convolutional layers (Conv2D) for feature extraction, followed by max pooling (MaxPooling2D) to reduce spatial dimensions. After flattening the feature maps, dense layers (Dense) with ReLU activation functions are employed for further feature processing. The output layer uses a sigmoid activation function to produce probabilities for binary classification."
      ],
      "metadata": {
        "id": "qEmbRq_ToZnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this early stopping definition, training stops if the validation loss function doesn't improve for 6 epochs and restore the weights to the values they had with the lower loss function."
      ],
      "metadata": {
        "id": "bOEgkcHtQ7hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition of early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "HCn1TsCkQnYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca-6Vrrn6BV1"
      },
      "outputs": [],
      "source": [
        "# CNN model\n",
        "CNN_model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=X_train.shape[1:]),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),  # relu as activation function in hidden layers\n",
        "    Dense(1, activation='sigmoid')  # 1 output class pneumonia (1) / normal (0), sigmoid as activation function in output layer\n",
        "])\n",
        "CNN_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "start_time = time.time()\n",
        "history = CNN_model.fit(X_train, y_train, epochs=100, validation_data=(X_validation, y_validation), callbacks=[early_stopping])\n",
        "end_time = time.time()\n",
        "\n",
        "# validation set\n",
        "y_pred_val_cnn = (CNN_model.predict(X_validation) > 0.5).astype(\"int32\")\n",
        "accuracy_val_cnn = accuracy_score(y_validation, y_pred_validation)\n",
        "#test set\n",
        "y_pred_test_cnn = (CNN_model.predict(X_test) > 0.5).astype(\"int32\") # convert the continuous predictions of a binary classification model into discrete class labels\n",
        "accuracy_test_cnn = accuracy_score(y_test, y_pred_cnn)\n",
        "\n",
        "# Processing time\n",
        "cnn_processing_time = end_time - start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These metrics assert that the CNN is working well with the data. The scores are evaluated on the validation set and are high."
      ],
      "metadata": {
        "id": "T3jG33neDvQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CNN:\")\n",
        "print(\"Accuracy CNN (validation):\", accuracy_val_cnn)\n",
        "print(\"Accuracy CNN (test):\", accuracy_test_cnn)\n",
        "print(\"Classification Report CNN:\")\n",
        "print(classification_report(y_validation, y_pred_val_cnn))\n",
        "print(\"Classification Report CNN:\")\n",
        "print(classification_report(y_test, y_pred_test_cnn))"
      ],
      "metadata": {
        "id": "sxdsBz6pJ1VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visual representation of accuracy and loss function through the epochs on the training set and on the validation set.\n",
        "\n",
        "The accuracy reached for both the sets over 95%.\n",
        "\n",
        "I represented also the loss function for both training and validation set. The loss on the first set decreased rapidly, while the low values on the validation one states that the model has good proprierties of generalization"
      ],
      "metadata": {
        "id": "RkrEcfOWEHpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.xlabel('Epochs')\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W9GJJH-aNQbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_cnn), annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Confusion Matrix - CNN (Test)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MMpC1qcAHE1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to apply Gaussian filter to the images and keep the labels\n",
        "def apply_gaussian_filter(X, y):\n",
        "    filtered_images = []\n",
        "    for image in X:\n",
        "        # Apply the filter to every image\n",
        "        filtered_image = cv2.GaussianBlur(image, (3, 3), 0)\n",
        "        filtered_images.append(filtered_image)\n",
        "    return np.array(filtered_images), np.array(y)\n",
        "\n",
        "# Example usage\n",
        "filtered_X_train, filtered_y_train = apply_gaussian_filter(X_train, y_train)\n",
        "filtered_X_validation, filtered_y_validation = apply_gaussian_filter(X_validation, y_validation)\n",
        "filtered_X_test, filtered_y_test = apply_gaussian_filter(X_test, y_test)\n",
        "\n",
        "# CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=X_train.shape[1:]),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),  # relu as activation function in hidden layers\n",
        "    Dense(1, activation='sigmoid')  # 1 output class pneumonia (1) / normal (0), sigmoid as activation function in output layer\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "start_time = time.time()\n",
        "\n",
        "# Training the model with filtered data\n",
        "history = model.fit(filtered_X_train, filtered_y_train, epochs=100, validation_data=(filtered_X_validation, filtered_y_validation), callbacks=[early_stopping])\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Predictions\n",
        "filtered_y_pred_cnn = (model.predict(filtered_X_test) > 0.5).astype(\"int32\") # convert the continuous predictions of a binary classification model into discrete class labels\n",
        "\n",
        "# Calculate accuracy\n",
        "filtered_accuracy_cnn = accuracy_score(filtered_y_test, y_pred_cnn)\n",
        "\n",
        "# Processing time\n",
        "filtered_cnn_processing_time = end_time - start_time\n"
      ],
      "metadata": {
        "id": "dvbBTmO9J88L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I compared the results obtained without and with filter using CNN, but there are not significant improvements, so I decided to continue with the orginal images."
      ],
      "metadata": {
        "id": "cZp9cwjyVp_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Without filter\n",
        "print(\"CNN without filter:\")\n",
        "print(\"Accuracy CNN (test):\", accuracy_score(y_test, y_pred_cnn))\n",
        "\n",
        "# With filter\n",
        "print(\"\\nCNN with filter:\")\n",
        "print(\"Accuracy CNN (test):\", filtered_accuracy_cnn)"
      ],
      "metadata": {
        "id": "Kso-IwOYO7zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to add some reguarization and retrain the model to have some improvements on the classification. I chose a L2 penalty."
      ],
      "metadata": {
        "id": "sLOq2P2hGB94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding some regularizers to the CNN\n",
        "CNN_model_reg = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=X_train.shape[1:]),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Re-training the model\n",
        "CNN_model_reg.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = CNN_model_reg.fit(X_train, y_train, epochs=100, validation_data=(X_validation, y_validation), callbacks=[early_stopping])\n",
        "cnn_accuracy = CNN_model_reg.evaluate(X_test, y_test)[1]"
      ],
      "metadata": {
        "id": "K1-1gzqYLg3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the graphic I can see that an improvement has been made. The loss function reaches lower values than before and the two functions for training and validation sets are one the same level after the first epoch."
      ],
      "metadata": {
        "id": "VQKmIBRUGSwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Bhg7aNuOOd4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XZGJrxnFL6vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix of the regularized predictions says that the misclassified images are less than before, when I used a CNN without L2 Penalty."
      ],
      "metadata": {
        "id": "kF9GC4SlHbiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_cnn), annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Confusion Matrix - CNN (Test)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nuBQuTcFv1-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **K-Nearest Neighbors**"
      ],
      "metadata": {
        "id": "2NfxUXgba3Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape every image in X_train to use it in KNN\n",
        "X_train_flat = np.array([image.reshape(-1) for image in X_train])\n",
        "\n",
        "# Reshape every image in X_test\n",
        "X_test_flat = np.array([image.reshape(-1) for image in X_test])\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {'n_neighbors': [3, 5, 7, 9, 11]}  # You can adjust the range of k values as needed\n",
        "\n",
        "# Create a k-nearest neighbors classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Perform cross-validation to find the best value of k\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5)  # 5-fold cross-validation\n",
        "grid_search.fit(X_train_flat, y_train)\n",
        "\n",
        "# Get the best value of k\n",
        "best_k = grid_search.best_params_['n_neighbors']\n",
        "\n",
        "print(\"Best value of k:\", best_k)\n",
        "\n",
        "# Create a k-NN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=best_k)  # You can adjust the number of neighbors (k) as needed\n",
        "\n",
        "# Train the classifier on the training data\n",
        "knn.fit(X_train_flat, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred_knn = knn.predict(X_test_flat)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
        "confusion_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
        "class_report_knn = classification_report(y_test, y_pred_knn)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(\"Accuracy:\", accuracy_knn)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix_knn)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report_knn)"
      ],
      "metadata": {
        "id": "SV_6ezCBWP6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix_knn, annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Confusion Matrix - CNN (Test)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GaY7jVDWYy0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic regression**"
      ],
      "metadata": {
        "id": "9lYDMvLnFhVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As logistic regression works with 1D data, flatten the features into a single vector for each data point."
      ],
      "metadata": {
        "id": "pFo22fLhhjiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape every image in X_train to use it in Logistic Regression Model\n",
        "X_train_flat = np.array([image.reshape(-1) for image in X_train])\n",
        "\n",
        "# Reshape every image in X_test\n",
        "X_test_flat = np.array([image.reshape(-1) for image in X_test])\n",
        "\n",
        "# Create a StandardScaler object and fit it to the training data\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform it\n",
        "X_train_scaled = scaler.fit_transform(X_train_flat)\n",
        "\n",
        "# Transform the testing data using the same scaler\n",
        "X_test_scaled = scaler.transform(X_test_flat)\n",
        "\n",
        "# Reshape y into a 1D array using reshape(-1)\n",
        "y_train_1d = y_train.reshape(-1)\n",
        "y_test_1d = y_test.reshape(-1)\n",
        "\n",
        "# Verify data are scaled correctly\n",
        "mean = X_train_scaled[0].mean(axis=0)\n",
        "print(mean)\n",
        "\n",
        "std = X_train_scaled[0].std(axis=0)\n",
        "print(std)\n",
        "\n",
        "print(y_train_1d)\n",
        "print(y_test_1d[1:10], '...', y_test_1d[-10:])"
      ],
      "metadata": {
        "id": "K5BWT4cihipS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a logistic regression model\n",
        "log_reg = LogisticRegression(solver='newton-cg')  # Newton's method for optimization. It's suitable for small to medium-sized datasets.\n",
        "\n",
        "# Train the model on the training data\n",
        "log_reg.fit(X_train_scaled, y_train_1d)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_log_reg = accuracy_score(y_test_1d, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test_1d, y_pred)\n",
        "class_report = classification_report(y_test_1d, y_pred)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(\"Accuracy:\", accuracy_log_reg)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "id": "Qnkb-emKAMDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model comparison**\n",
        "I plotted the accuracy for the previous models. The highest values is the CNN one, even if all models have an accuracy over 90%."
      ],
      "metadata": {
        "id": "xUkIaU-ah2rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy scores for each model\n",
        "models = ['Random Forest', 'CNN', 'KNN', 'Logistic Regression']\n",
        "accuracy_scores = [rf_accuracy, accuracy_cnn, accuracy_knn, accuracy_log_reg]  # Example accuracy scores\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(models, accuracy_scores, color = [\"#ffd1dc\", \"#d3f0f7\", \"#b2d0c7\", \"#e6e6fa\"])\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Comparison of Different Models')\n",
        "plt.ylim(0.7, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z-SAg_jnGKcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to store predicted probabilities for each model\n",
        "predicted_probs = {\n",
        "    'Random Forest': rf_model_best.predict_proba(X_test_scaled)[:, 1],\n",
        "    'CNN': CNN_model_reg.predict(X_test),\n",
        "    'KNN': knn.predict_proba(X_test_scaled)[:, 1],\n",
        "    'Logistic Regression': log_reg.predict_proba(X_test_scaled)[:, 1]\n",
        "}"
      ],
      "metadata": {
        "id": "zST9SENsXWas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_predictions = {\n",
        "    'Random Forest': (predicted_probs['Random Forest'], y_test),\n",
        "    'CNN': (predicted_probs['CNN'], y_test),\n",
        "    'KNN': (predicted_probs['KNN'], y_test),\n",
        "    'Logistic Regression': (predicted_probs['Logistic Regression'], y_test)\n",
        "}\n",
        "\n",
        "# Plot ROC curve for each model\n",
        "plt.figure(figsize=(8, 6))\n",
        "for model_name, (predicted_probs, y_test) in model_predictions.items():\n",
        "    if isinstance(predicted_probs, tuple):  # If predicted_probs is a tuple\n",
        "        predicted_probs = predicted_probs[0]  # Get the first element of the tuple\n",
        "    if predicted_probs.ndim == 1:  # If predicted_probs is a 1D array (binary classification)\n",
        "        fpr, tpr, _ = roc_curve(y_test, predicted_probs)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
        "    else:  # If predicted_probs is a 2D array (multiclass classification)\n",
        "        # Assuming we have binary classification for each class vs the rest\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        for i in range(predicted_probs.shape[1]):\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_test, predicted_probs[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "            plt.plot(fpr[i], tpr[i], lw=2, label=f'{model_name} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "# Plot the random guessing line\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "\n",
        "# Set plot labels and title\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "je5VM0C1VmVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROC Curve Analysis**\n",
        "\n",
        "ROC curves assessed model performance (Random Forest, CNN, KNN, Logistic Regression). The CNN achieved the highest AUC, indicating better differentiation between normal and pneumonia cases in the MedMNIST dataset. Logistic Regression works nearly as good ad the CNN. KNN and Random Forest showed more room for improvement as their curves approached the random classifier line."
      ],
      "metadata": {
        "id": "u9DpXFD1cJcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **F1-score**\n",
        "F1-score is particularly useful for imbalanced datasets, where there might be a significant difference in the number of positive and negative examples. This is the case of MedMNIST Pneumonia data set, in which we have more pneumonia images than normal ones, as showed at the beginning."
      ],
      "metadata": {
        "id": "cMDshR15osZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions for the test set\n",
        "rf_probs_test = rf_model_best.predict_proba(X_test.reshape(len(X_test), -1))[:, 1]\n",
        "\n",
        "# Add Random Forest predictions to the model_predictions dictionary\n",
        "model_predictions['Random Forest'] = (rf_probs_test, y_test)\n",
        "\n",
        "# Calculate F1 score for each model\n",
        "model_f1_scores = {}\n",
        "for model_name, (predicted_probs, y_test) in model_predictions.items():\n",
        "    y_pred = (predicted_probs > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    model_f1_scores[model_name] = f1\n",
        "\n",
        "# Plot F1 score for each model\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(model_f1_scores.keys(), model_f1_scores.values(), color='skyblue')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('F1 Score for Each Model')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "99n0hupzZPaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the plot, I can assert that CNN is the model that achieved the best results, as it has the highest F1-score. The other models have high scores too. The worst one is KNN, even if its score is near 90%."
      ],
      "metadata": {
        "id": "WdzG8C8Do1g1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zu9idWZ8lyeX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Am__M4pTXRbSndz2xE8MPReAtbo3nOYl",
      "authorship_tag": "ABX9TyPjYlDX8/qJP3HXJhLo2ekP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}